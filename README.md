# DNN_Day2

提出レポート
はるはる
深層学習:Day2


Section1: 勾配消失問題
・勾配消失問題とは、誤差逆伝播法が下位層に進んでいくに連れて、勾配がどんどん緩やかになっていき、それによって、勾配降下法による更新では下位層のパラメータはほとんど変わらず、訓練は最適値に収束しなくなること。
深層学習は、学習を通して誤差を最小にするネットワークを作成すること勾配降下法を利用してパラメータを最適化したい

勾配消失問題の解決法
①活性化関数の選択
・シグモイド関数は大きな値では出力の変化が微小なため、勾配消失問題を引き起こす事があった。
・ReLU関数は、今最も使われている活性化関数で、勾配消失問題の回避とスパース化に貢献することで良い成果をもたらしている。
②重みの初期値設定
・Xavier(ザビエル)は初期値を設定する際の活性化関数で、重みの要素を、前の層のノード数の平方根で除算した値。ReLU関数、シグモイド（ロジスティック）関数、双曲線正接関数に使うことで勾配消失問題を解決しやすい。
・Heは重みの要素を、前の層のノード数の平方根で除算した値に対し√２をかけ合わせた値PythonS1)-2日本語27
を使う初期値の設定方法で、ReLU関数に使うと勾配消失問題を解決する。
③バッチ正規化
・バッチ正規化とは、ミニバッチ単位で入力値のデータの偏りを抑制する手法
活性化関数に値を渡す前後に、バッチ正規化の処理をした層を加える

データについて
・日本はデータが少ない
・データの集め方について学んでおく

【確認テスト】
[p12]
Q,連鎖律の原理を使い、dz/dxを求めよ。
A,
z = t2
t = x +y
dz/dx = (dz/dt)・(dt/dx)=2t・1 = 2(x +y)
■考察
簡単な微分の復習でした。

[p20]
Q,シグモイド関数を微分した時、入力値が0の時に最大値をとる。その値として正しいものを選択肢から選べ。
（1）0.15（2）0.25（3）0.35（4）0.4520
A,(2)
■考察
入力値が0の時、シグモイド関数は0.5
f'(u) = (1-sigmoid(u))・sigmoid(u) = (1-0.5)・0.5 = 0.25

[p28]
Q,重みの初期値に0を設定すると、どのような問題が発生するか。簡潔に説明せよ。
A,全ての値が同じ値で伝わるためパラメータのチューニングは行われなくなる。
■考察
そもそも深層学習の概念が阻害されてしまうから。

[p31]
一般的に考えられるバッチ正規化の効果を2点挙げよ。
A, 計算の高速化、勾配消失問題が起きづらくなる
■考察
計算の高速化については、データに関するばらつきが抑制されるから。


Section2: 学習率最適化手
学習率の値が大きい場合→最適値にいつまでもたどり着かず発散してしまう。
学習率の値が小さい場合→発散することはないが、小さすぎると収束するまでに時間がかかってしまう。大域局所最適値に収束しづらくなる。
学習率の決め方は、初期の学習率を大きく設定し、徐々に学習率を小さくしていくか、パラメータ毎に学習率を可変させるという選択肢がある。4つの学習率最適化手法のどれかを利用して学習率を最適化する。

①モメンタム
誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する誤差をパラメータで微分したものと学習率の積を減算する
(諸富さん的には、大体0.05〜0.09が一般的)
新しくVを設定する。使いこなせれば優秀な学習率。
メリット→1,局所的最適解にはならず、大域的最適解となる。2,谷間についてから最も低い位置(最適値)にいくまでの時間が早い。

②AdaGrad
誤差をパラメータで微分したものと再定義した学習率の積を減算する。
メリット→勾配の緩やかな斜面に対して、最適値に近づける
課題→学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった。

③RMSProp
AdaGradの課題を解決する方法
誤差をパラメータで微分したものと再定義した学習率の積を減算する。
メリット→局所的最適解にはならず、大域的最適解となる。(AdaGradと比較して)
ハイパーパラメータの調整が必要な場合が少ない

④Adam
モメンタムの過去の勾配の指数関数的減衰平均と、RMSPropの過去の勾配の2乗の指数関数的減衰平均のそれぞれを孕んだ最適化アルゴリズム。
メリットとは→モメンタムとRMSPropのいいところ取り

・シンギュラリティー(技術的特異点)
脳のパーツごとにモジュールはできているが、"欲"が定義されていない。
それが定義された時にできるのではないか？
何が作りたいかを考えて、イマジネーションを活かす。
技術革新の記事をチェックして、学習意欲を掻き立てる


【確認テスト】
[p47]
Q,モメンタム・AdaGrad・RMSPropの特徴をそれぞれ簡潔に説明せよ。
A,
モメンタム→収束するまでが早い
AdaGrad→緩やかな斜面でも最適値に近づける
RMSProp→パラメータの調整が少なくて済む
■考察
なぜAdaGrad・RMSPropを習う前に、確認テストが行われたのか謎でした。


Section3: 過学習
過学習とはテスト誤差と訓練誤差とで学習曲線が乖離すること。特定の訓練サンプルに対して、特化して学習してしまうこと。
原因としては、パラメータの数が多い、パラメータの値が適切でない、ノードが多いなどがある。
課題は、ネットワークの自由度(層数、ノード数、パラメータの値など)が高い
解決策は正則化手法
正則化とは、ネットワークの自由度(層数、ノード数、パラメータの値など)を制約すること。

Weight decay(荷重減衰)
重みが大きい値をとることで、過学習が発生することがある。
学習させていくと、重みにばらつきが発生する。
重みが大きい値は、学習において重要な値であり、重みが大きいと過学習が起こる
解決策→誤差に対して、正則化項を加算することで、重みを抑制する。
過学習がおこりそうな重みの大きさ以下で重みをコントロールし、かつ重みの大きさにばらつきを出す必要がある


①L1正則化、L2正則化
誤差関数に、pノルムを加える。p = 1の場合、L1正則化と呼ぶ。p = 2の場合、L2正則化と呼ぶ。

②ドロップアウト
最も代表的な過学習の解決手法。ノードの数が多いという課題を、ランダムにノードを削除して学習させることで解消する。
メリット→データ量を変化させずに、異なるモデルを学習させていると解釈できる


【確認テスト】
[p63]
Q,機械学習で使われる線形モデル(線形回帰、種成分分析…etc)の正則化は、
モデルの重みを制限することで可能となる。
前述の線形モデルの正則化手法の中にリッジ回帰という手法があり、
その特徴として正しいものを選択しなさい。
A,(a)
■考察
"そういうもの"と覚える。
(b)→線形回帰
(c)バイアス項は正則化されない
(d)隠れ層→誤差関数

[p68]
Q, 下図について、L1正則化を表しているグラフはどちらか答えよ。
A,右
■考察
L1正則化は0が取れるので、スパースの解決策になる。
シグモイド関数は計算の表記上は0でも0.001だったりする。ReLU関数によって0が取れるようになったのと同じ。
L2正則化は精度向上に効果がある。
図の円と四角の接点は、誤差項と正則化項の接点

【例題チャレンジ】
[p69]5.L2パラメータ正則化
A,(4)param
■考察
L2ノルムは、||param||^2なのでその勾配が誤差の勾配に加えられる。つまり、2 * paramであるが、係数2は正則化の係数に吸収されても変わらないのでparamが正解である。 

[p71] 6.L1パラメータ正則化
A,(3)np.sign(param)
L1ノルムは、|param|なのでその勾配が誤差の勾配に加えられる。つまり、sign(param)である。signは符号関数 (0の場合は0が返ってきて、マイナスの値には-1が返ってくる) である。

[p73] 7.データ集合の拡張
A, image[top:bottom, left:right, :] 
■考察
imageの形式が(縦幅, 横幅, チャンネル)であるのも考慮する。
topからbottom、leftからrightをとる


Section4: 畳み込みニューラルネットワークの概念

・畳み込みニューラルネットワークは、入力層から入力された入力値が畳み込み層とプーリング層を通り、バイアスが足されて、活性化関数がかけられて、出力値となる。
・畳み込み層では、画像の場合、縦、横、チャンネルの3次元のデータをそのまま学習し、次に伝えることができるので、3次元の空間情報も学習できる。
・パディングは、入力画像フィルターの周りを数字で囲む手法。0パディングが有名。
・ストライドは計算の範囲のスライドの幅。
・全結合層のデメリット→画像の場合、縦、横、チャンネルの3次元データだが、1次元のデータとして処理されるので、RGBの各チャンネル間の関連性が、学習に反映されないこと。
・プーリング層では、入力画像から対象領域のMax値または、平均値を取得し、出力値を算出する。

【確認テスト】
[p100] 
Q,サイズ6×6の入力画像を、サイズ2×2のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおストライドとパディングは1とする。
A,7×7
OH=(H+2P-FH)/S+1=  (6+2・1-2)/1+1=7 
OW=(H+2P-FH)/S+1=  (6+2・1-2)/1+1=7 
■考察
公式の仕組みをちゃんと理解する。

H=Height, W=wide
P=パディング
S=ストライド

Section5: 最新のCNN
・AlexNet
2012年に開かれた、世界的に有名な画像認識コンペティションで2位に大差をつけて優勝。5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成されるモデル。
サイズ4096の全結合層の出力にドロップアウトを使用することで過学習を防ぐ。
